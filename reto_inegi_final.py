# -*- coding: utf-8 -*-
"""Reto_INEGI_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qHbR-0M0JEuES293Vsm1kkVIi97lHS2E

**INTEGRACIÓN DE DATOS**
"""

from google.colab import drive
drive.mount('/content/drive')

folder_path = "/content/drive/My Drive/Reto INEGI/"

# Importamos librerias
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
import numpy as np

# Leemos bases de datos que vienen del Drive
ruta = "/content/drive/My Drive/Reto INEGI/"
df_inegi = pd.read_csv(ruta + "datosInegi2020.csv")
df_coneval = pd.read_csv(ruta + "indicadores de pobreza municipal_2020.csv", encoding= "latin-1")

# Limpiamos datos y hacemos un merge de nuestras bases de datos
df_inegi = df_inegi.loc[df_inegi["NOM_LOC"] == "Total del Municipio"]
df_inegi.rename(columns={"NOM_MUN": "municipio", "ENTIDAD": "clave_entidad"}, inplace=True)
df_inegi.reset_index(drop=True, inplace=True)
df_inegi

nueva_lista = []
for i in range (df_inegi.shape[0]):
  if int(df_inegi["MUN"][i]) < 10:
    nueva_lista.append(str(df_inegi["clave_entidad"][i]) + "00" + str(df_inegi["MUN"][i]))
  elif int(df_inegi["MUN"][i]) < 100 :
    nueva_lista.append(str(df_inegi["clave_entidad"][i]) + "0" + str(df_inegi["MUN"][i]))
  else:
    nueva_lista.append(str(df_inegi["clave_entidad"][i]) + str(df_inegi["MUN"][i]))

df_inegi["clave_municipio"] = nueva_lista

df_inegi['clave_municipio'] = df_inegi['clave_municipio'].astype(str)
df_coneval['clave_municipio'] = df_coneval['clave_municipio'].astype(str)
df_coneval = df_coneval[["pobreza_e_pob", "ic_cv_pob", "clave_municipio"]]
df = pd.merge(df_inegi,df_coneval, on= "clave_municipio")

"""**PREPARACIÓN DE LOS DATOS**

## Selección de variables
"""

df = df.applymap(lambda x: str(x).replace(',', ''))

data = df.apply(pd.to_numeric, errors = "coerce")
data = data.dropna(subset=["pobreza_e_pob", "ic_cv_pob"])
data.dropna(inplace=True, axis = 1)
data.drop(columns= ["clave_entidad", "clave_municipio", "MUN", "LOC"], inplace = True)

variables = data.columns.tolist()
predecir = ["pobreza_e_pob", "ic_cv_pob"]

data.shape

"""## Para el indicador de pobreza extrema"""

import numpy as np
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import StandardScaler
# Convertir el dataframe a un array de NumPy
array = data.values

pobreza_e_pob_index = variables.index('pobreza_e_pob')
# Definir X y Y basados en el número de variables
X = array[:, :len(variables)-2]  # Todas las columnas de entrada
Y = array[:, pobreza_e_pob_index]   # Primera columna de salida (pobreza_e)

# Escalar X con StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar SelectKBest con f_regression (correlación de Pearson)
selector = SelectKBest(score_func=f_regression, k=10)
X_selected = selector.fit_transform(X_scaled, Y)

# Ver los puntajes de cada variable
print("Puntuaciones de las variables:", selector.scores_)

interes1_p_d = {}
for i in range(0,len(selector.scores_)):
  interes1_p_d[variables[i]] = selector.scores_[i]
claves = sorted(interes1_p_d, key=interes1_p_d.get, reverse = True)

interes1_p = claves[0:20]

print(interes1_p)

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

unique, counts = np.unique(Y, return_counts=True)
print("Distribución de Y:", dict(zip(unique, counts)))

#model = LogisticRegression(max_iter=5000, solver='liblinear')

# Aplicar RFE con LogisticRegression
#rfe = RFE(model, n_features_to_select=20)
#fit = rfe.fit(X_scaled, Y)

 #Imprimir resultados
#print("Num Features Selected:", fit.n_features_)
#print("Selected Features (True=Selected):", fit.support_)
#print("Feature Ranking (1=Most Important):", fit.ranking_)

resultados_rfe =[
    257, 246, 249, 157,  64,  90, 256, 244, 248, 255, 242, 245, 254, 239, 237, 250, 236, 234,
    247, 233, 224, 182, 119, 104, 195, 162, 161, 203, 177, 174, 180, 106, 115, 181, 142, 109,
    193, 173, 164, 222, 215, 206, 188,  21, 219, 243, 211, 189, 145, 152, 187, 143, 150, 186,
    153, 147, 201, 167, 159, 175, 136, 124, 168, 141, 131, 176, 137, 117, 172, 148, 125, 165,
    118, 129, 171, 130, 138, 163, 126, 116, 154, 110,  92, 144, 100,  74, 128,  82,  65, 122,
    76,  56,  95,  51,  24,  91,  19,   1,  78,   1,   1,   1, 240, 227, 229, 169, 114,  37,
    253, 241, 238, 170,  85,  61, 208, 123, 139, 179,  49,  50, 178,  25,  79, 212, 140, 194,
    63, 199,  73, 133, 120,  34,   1,  20,  11,  52,  39, 105,  54,  17,   9,  60,   1,   1,
    7, 235, 108,  23,  55,  97,   5,  22, 113,  29,  30, 135,  83,  59, 132,  67,  36,  96,
    4,   3, 111,   1,  44, 112,  46,   1,  93,   6,  48, 103,  75,  38, 101,   1,  42, 149,
    77,  94, 183, 166, 146,  89,   1,   1, 190, 102, 204, 151, 158,  72, 185,  81, 200,  88,
    27,   2,  14, 184,  70,  26,  99, 156,   1,  66,  53,  57, 191, 209, 127, 217,  31, 134,
    43, 232, 107, 192, 251,  69, 205, 207, 230, 198, 216, 228, 231,   1,  10, 252,   1,   1,
    225,   1,  87, 202,   1,  16, 210, 226,  35, 223,  62,  33,  32,  47, 214,  18, 221,  15,
    220,  68, 218,  45,  12, 197, 155,  86, 121,  13,   1,  84, 213, 160,  41, 196,  58,   8,
    98,  71,  28,   1,  80,  40
]

interes2_p = []
for i in range(len(resultados_rfe)):
    if resultados_rfe[i] == 1:
        interes2_p.append(variables[i])
interes2_p

from sklearn.linear_model import Ridge
# Aplicar Ridge Regression
ridge = Ridge(alpha=5.0)
ridge.fit(X_scaled, Y)

def pretty_print_coefs(coefs, names = None, sort = False):
    if names == None:
        names = ["X%s" % x for x in range(len(coefs))]
    lst = zip(coefs, names)
    if sort:
        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))
    return " + ".join("%s * %s" % (round(coef, 3), name)
                                   for coef, name in lst)

print ("Ridge model:", pretty_print_coefs(ridge.coef_))
a = pretty_print_coefs(ridge.coef_)

interes3_p = []
for i in range(0,len(ridge.coef_)):
  if ridge.coef_[i] > 1000: #No hay mayores de 5000
    interes3_p.append(variables[i])
print(interes3_p)

interesp_list = [interes1_p, interes2_p, interes3_p]
top_variables1_d = {}
for interes in interesp_list:
  for variable in interes:
    if variable in top_variables1_d:
      top_variables1_d[variable] += 1
    else:
      top_variables1_d[variable] = 1
top_variables1 = [clave for clave in top_variables1_d if top_variables1_d[clave] >= 1]
print(top_variables1)

top_variables1_d

variables1 = top_variables1[:]
data1 = data[variables1 + [predecir[0]]].copy()
data1.shape

"""**Nuestras variables:**
*   TVIVHAB = Total de viviendas habitadas
*   PRO_OCUP_C = Promedio de ocupantes por cuarto en viviendas particulares habitadas
*   PDESOCUP = Población de 12 años y más desocupada
*   VPH_AGUAFV = Viviendas particulares habitadas que no disponen de agua entubada en el ámbito de la vivienda
*   PEA = Población de 12 años y más económicamente activa
*   POCUPADA = Población de 12 años y más ocupada
*   VPH_NODREN = Viviendas particulares habitadas que no disponen de drenaje
*   VPH_EXCSA = Viviendas particulares habitadas que disponen de excusado o sanitario

## Para el Indicador de Carencia por Calidad y Espacios de la vivienda
"""

ic_cv_pob_index = variables.index('ic_cv_pob')
# Definir X y Y basados en el número de variables
X2 = array[:, :len(variables)-2]  # Todas las columnas de entrada
Y2 = array[:, ic_cv_pob_index]   # Primera columna de salida (pobreza_e)

# Escalar X con StandardScaler
scaler = StandardScaler()
X2_scaled = scaler.fit_transform(X2)

# Aplicar SelectKBest con f_regression (correlación de Pearson)
selector = SelectKBest(score_func=f_regression, k=10)
X2_selected = selector.fit_transform(X2_scaled, Y2)

# Ver los puntajes de cada variable
print("Puntuaciones de las variables:", selector.scores_)

interes1_c_d = {}
for i in range(0,len(selector.scores_)):
  interes1_c_d[variables[i]] = selector.scores_[i]
claves = sorted(interes1_c_d, key=interes1_c_d.get, reverse = True)

interes1_c = claves[0:20]

print(interes1_c)

unique, counts = np.unique(Y2, return_counts=True)
print("Distribución de Y:", dict(zip(unique, counts)))


#model = LogisticRegression(max_iter=5000, solver='liblinear')

# Aplicar RFE con LogisticRegression

#rfe = RFE(model, n_features_to_select=20)
#fit = rfe.fit(X2_scaled, Y2)

# Imprimir resultados

#print("Num Features Selected:", fit.n_features_)

#print("Selected Features (True=Selected):", fit.support_)

#print("Feature Ranking (1=Most Important):", fit.ranking_)

resultados =  [257, 247, 249, 142, 69, 88, 256, 245, 248, 255, 242, 243, 254, 240, 236, 250, 238, 233,
         246, 234, 225, 183, 109, 112, 201, 174, 160, 204, 180, 170, 175, 103, 120, 179, 135, 116,
         190, 162, 165, 223, 216, 207, 188, 16, 220, 244, 209, 189, 150, 159, 194, 148, 152, 186,
         154, 146, 196, 171, 157, 178, 130, 134, 172, 141, 121, 173, 137, 126, 167, 143, 113, 161,
         123, 118, 177, 136, 139, 163, 124, 107, 155, 111, 85, 140, 96, 74, 127, 84, 59, 131,
         80, 64, 101, 54, 20, 89, 27, 1, 76, 1, 1, 1, 239, 227, 231, 169, 115, 49,
         251, 241, 237, 164, 81, 41, 208, 125, 138, 181, 52, 53, 176, 26, 79, 212, 144, 192,
         58, 202, 75, 133, 128, 56, 1, 1, 22, 42, 35, 117, 44, 24, 11, 61, 1, 1,
         15, 235, 104, 47, 5, 97, 1, 19, 110, 30, 28, 129, 78, 62, 132, 43, 72, 99,
         1, 1, 106, 8, 37, 122, 57, 1, 91, 3, 51, 102, 71, 60, 98, 1, 46, 145,
         77, 95, 185, 168, 149, 93, 1, 1, 187, 100, 203, 147, 156, 65, 184, 83, 197, 90,
         23, 9, 4, 182, 55, 25, 105, 158, 18, 66, 40, 50, 198, 211, 119, 214, 29, 151,
         38, 230, 108, 191, 252, 70, 205, 206, 232, 193, 218, 228, 229, 1, 7, 253, 6, 1,
         224, 2, 82, 200, 1, 10, 210, 226, 32, 222, 67, 34, 36, 45, 215, 21, 221, 17,
         219, 68, 217, 39, 12, 195, 166, 94, 114, 13, 1, 86, 213, 153, 33, 199, 63, 14,
         92, 73, 31, 1, 87, 48]
variables3_1 = []
for i in range(len(resultados)):
    if resultados[i] == 1:
        variables3_1.append(variables[i])

ridge = Ridge(alpha=5.0)
ridge.fit(X2_scaled,Y2)
print ("Ridge model:", pretty_print_coefs(ridge.coef_))

interes3_c = []
for i in range(0,len(ridge.coef_)):
  if ridge.coef_[i] > 1000: #No hay mayores de 5000
    interes3_c.append(variables[i])
print(interes3_c)

interesc_list = [interes1_c, interes3_c,variables3_1]
top_variables2_d = {}
for interes in interesp_list:
  for variable in interes:
    if variable in top_variables2_d:
      top_variables2_d[variable] += 1
    else:
      top_variables2_d[variable] = 1
top_variables2 = [clave for clave in top_variables2_d if top_variables2_d[clave] >= 1]
print(top_variables2)

top_variables2_d

variables2 = top_variables2[:]
data2 = data[variables2 + [predecir[1]]].copy()
data2.shape

print(top_variables1)
print(top_variables2)

"""**Variables:**
*   VPH_PISOTI = Viviendas particulares habitadas con piso de tierra
*   VPH_1CUART = Viviendas particulares habitadas con sólo un cuarto
*   PDESOCUP = Población de 12 años y más desocupada
*   VPH_S_ELEC = Viviendas particulares habitadas que no disponen de energía eléctrica
*   VPH_NODREN = Viviendas particulares habitadas que no disponen de drenaje
*   VPH_EXCSA = Viviendas particulares habitadas que disponen de excusado o sanitario
*   VPH_PISODT = Viviendas particulares habitadas con piso de material diferente de tierra

### Limpieza de datos primer indicador
"""

original = data1.shape
data1 = data1.drop_duplicates() #Eliminar duplicados

print(f"Se han eliminado {original[0] - data1.shape[0]} datos duplicados") #Comprobar si se elimino algun duplicado

#df_select.to_csv( ruta + "datos_limpios.csv")
#df_select[variables] = df_select[variables].astype(int)
data1[variables1 + [predecir[0]]] = data1[variables1 + [predecir[0]]]
data1.dtypes
data1

"""### Trabajar con valores Nan"""

original = data1.shape
data1.isnull().sum()

#Eliminar valores nulos
data1.dropna(inplace=True)
data1.shape
print(f"Se han eliminado {original[0] - data1.shape[0]} filas con datos nulos")

data1[predecir[0]].describe()

plt.boxplot(data1)
plt.boxplot(data1)
plt.show()

data1 = data1[variables1]

Q1 = data1.quantile(0.25)
Q2 = data1.median()
Q3 = data1.quantile(0.75)
IQR = Q3 - Q1
outliers = ((data1 < (Q1 - 1.5 * IQR)) | (data1 > (Q3 + 1.5 * IQR)))
print(f"Cuartiles:\n{pd.DataFrame({'Q1': Q1, 'Mediana': Q2, 'Q3': Q3, 'IQR': IQR})}")
print(f"\nNúmero de outliers por variable:\n{outliers.sum()}")

"""### Limpieza de datos segundo indicador"""

original = data2.shape
df_select2 = data2.drop_duplicates() #Eliminar duplicados

print(f"Se han eliminado {original[1] - data2.shape[1]} datos duplicados") #Comprobar si se elimino algun duplicado

data2[variables2 + [predecir[1]]] = data2[variables2 + [predecir[1]]].apply(pd.to_numeric, errors = "coerce")
data2.dtypes
data2

"""### Trabajar con valores Nan"""

original = data2.shape
data2.isnull().sum()

#Eliminar valores nulos
data2.dropna(inplace=True)
data2.shape
print(f"Se han eliminado {original[1] - data2.shape[1]} filas con datos nulos")

data2[predecir[1]].describe()

"""## Transformacion de Datos"""

data1["index"] = data1.index
data2["index"] = data2.index
df_limpio = pd.merge(data1,data2, on= "index")
df_limpio.to_csv(ruta + "Df_limpio.csv")

"""### Análisis de correlación de las variables elegidas"""

plt.figure(figsize=(100, 100))
sns.heatmap(data1.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Mapa de Calor - Correlación de Variables con Indicador de Pobreza")
plt.show()

plt.figure(figsize=(100, 100))
sns.heatmap(data2.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Mapa de Calor - Correlación de Variables con Indicador de Carencia de Vivienda")
plt.show()

import pandas as pd
import numpy as np

# Calcular la matriz de correlación
corr_matrix = data1.corr()

# Obtener pares de variables con correlación mayor a 0.90 (sin contar la diagonal)
threshold = 0.90  # Umbral de correlación
high_corr_pairs = []

for i in range(len(corr_matrix.columns)):
    for j in range(i + 1, len(corr_matrix.columns)):  # Evita duplicados y la diagonal
        if abs(corr_matrix.iloc[i, j]) > threshold:
            var1 = corr_matrix.columns[i]
            var2 = corr_matrix.columns[j]
            corr_value = corr_matrix.iloc[i, j]
            high_corr_pairs.append((var1, var2, corr_value))

# Convertir en DataFrame para fácil visualización
high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlación'])

# Mostrar resultados
print(high_corr_df)

import pandas as pd

# Asegurarse de que 'pobreza_e_pob' esté en data1
data1["pobreza_e_pob"] = data["pobreza_e_pob"]

# Calcular la matriz de correlación
corr_matrix = data1.corr()

# Umbral de correlación
threshold = 0.90

# Listas para guardar variables
variables_mantenidas = list(data1.columns)
variables_eliminadas = []

# Variable protegida (no debe eliminarse)
variables_protegida = ["pobreza_e_pob"]  # 'ic_cv_pob' no está en data1

# Encontrar variables con alta correlación
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i + 1, len(corr_matrix.columns)):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            var1 = corr_matrix.columns[i]
            var2 = corr_matrix.columns[j]
            high_corr_pairs.append((var1, var2))

# Convertir a DataFrame
high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2'])

# Eliminar variables correlacionadas, excepto la protegida
for var in high_corr_df["Variable 2"].unique():
    if var in variables_mantenidas and var not in variables_protegida:
        variables_mantenidas.remove(var)
        variables_eliminadas.append(var)

# DataFrames para visualizar
df_eliminadas = pd.DataFrame({'Variables Eliminadas': variables_eliminadas})
df_mantenidas = pd.DataFrame({'Variables Mantenidas': variables_mantenidas})

# Mostrar resultados
print("Variables eliminadas:")
print(df_eliminadas)

print("\nVariables mantenidas:")
print(df_mantenidas)

# Revisión Lógica sobre las variables eliminadas
variables_lógicas_pobreza = ["PSINDER","PCLIM_MOT2", "PDESOCUP", "pobreza_e_pob"]

# 1. Eliminar de variables_eliminadas y agregar a variables_mantenidas
for var in variables_lógicas_pobreza:
    if var in variables_eliminadas:
        variables_eliminadas.remove(var)
        if var not in variables_mantenidas:  # Evitar duplicados
            variables_mantenidas.append(var)

# 2. Actualizar DataFrames
df_eliminadas = pd.DataFrame({'Variables Eliminadas': variables_eliminadas})
df_mantenidas = pd.DataFrame({'Variables Mantenidas': variables_mantenidas})

# 3. Eliminar filas específicas por valor en df_mantenidas (opcional)
# Si necesitas eliminar filas por su valor, usa este enfoque:
valores_a_eliminar = ["index"]  # Valores a eliminar
df_mantenidas = df_mantenidas[~df_mantenidas['Variables Mantenidas'].isin(valores_a_eliminar)]

print("\nVariables mantenidas:")
print(df_mantenidas)

# Eliminar del DataFrame las columnas que aparecen en la lista de eliminadas
data_clean_pobreza = data1.drop(columns=variables_eliminadas)

# Verificar las nuevas dimensiones del DataFrame
print(data_clean_pobreza.shape)
data_clean_pobreza.columns.tolist()

plt.figure(figsize=(100, 100))
sns.heatmap(data_clean_pobreza.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Mapa de Calor - Correlación de Variables con Indicador de Pobreza Extrema")
plt.show()

import numpy as np
import pandas as pd
from sklearn.feature_selection import mutual_info_regression

# Convertimos el DataFrame a un array de numpy
X = np.array(data_clean_pobreza)

# Número de características
n_features = X.shape[1]

# Matriz para almacenar la información mutua
mi_matrix = np.zeros((n_features, n_features))

# Iteramos sobre cada par de variables
for i in range(n_features):
    for j in range(i + 1, n_features):
        # Calculamos la información mutua entre la variable i y la j
        mi = mutual_info_regression(X[:, [i]], X[:, j])  # mutual_info_regression espera una matriz 2D
        mi_matrix[i, j] = mi
        mi_matrix[j, i] = mi  # La matriz es simétrica

# Convertimos la matriz a un DataFrame con los nombres de las columnas originales
mi_df = pd.DataFrame(mi_matrix, index=data_clean_pobreza.columns, columns=data_clean_pobreza.columns)

# Mostrar la matriz de información mutua
print("Matriz de información mutua:")
print(mi_df)

# Umbral de información mutua
umbral = 0.90

# Filtrar los pares con información mutua mayor al umbral
pares_alto_mi = [(var1, var2, mi_df.loc[var1, var2])
                 for var1 in mi_df.index for var2 in mi_df.columns
                 if var1 != var2 and mi_df.loc[var1, var2] > umbral]

# Convertir los resultados en un DataFrame
df_pares_alto_mi = pd.DataFrame(pares_alto_mi, columns=["Variable 1", "Variable 2", "Información Mutua"]).drop_duplicates()

# Imprimir solo la tabla filtrada
if not df_pares_alto_mi.empty:
    print("\nPares con Información Mutua mayor a 0.90:")
    print(df_pares_alto_mi)
else:
    print("\nNo se encontraron pares con MI > 0.90.")

# Eliminar la columna "P8A14AN" si existe en el DataFrame
data_clean_pobreza = data_clean_pobreza.drop(columns=["P8A14AN", "index"])

# Mostrar el DataFrame actualizado
data_clean_pobreza.head(11)

data_clean_pobreza.shape

from sklearn.preprocessing import MinMaxScaler

# Aplicar transformación logarítmica
data_clean_pobreza["pobreza_e_pob"] = np.log1p(data_clean_pobreza["pobreza_e_pob"])

# Normalización Min-Max
scaler = MinMaxScaler()
data_clean_pobreza["pobreza_e_pob"] = scaler.fit_transform(
    data_clean_pobreza["pobreza_e_pob"].values.reshape(-1, 1)
)

#Distribución
plt.figure(figsize=(8,5))
sns.histplot(data_clean_pobreza["pobreza_e_pob"], bins=30, kde=True)
plt.title("Distribución de la Variable Objetivo")
plt.show()

data_clean_pobreza["pobreza_e_pob"].describe()

import pandas as pd

# Asegurarse de que 'ic_cv_pob' esté en data2
data2["ic_cv_pob"] = data["ic_cv_pob"]

# Calcular la matriz de correlación
corr_matrix = data2.corr()

# Umbral de correlación
threshold = 0.90

# Listas para guardar variables
variables_mantenidas = list(data2.columns)
variables_eliminadas = []

# Variable protegida (no debe eliminarse)
variables_protegida = ["ic_cv_pob"]

# Encontrar variables con alta correlación
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i + 1, len(corr_matrix.columns)):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            var1 = corr_matrix.columns[i]
            var2 = corr_matrix.columns[j]
            high_corr_pairs.append((var1, var2))

# Convertir a DataFrame
high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2'])

# Eliminar variables correlacionadas, excepto la protegida
for var in high_corr_df["Variable 2"].unique():
    if var in variables_mantenidas and var not in variables_protegida:
        variables_mantenidas.remove(var)
        variables_eliminadas.append(var)

# DataFrames para visualizar
df_eliminadas = pd.DataFrame({'Variables Eliminadas': variables_eliminadas})
df_mantenidas = pd.DataFrame({'Variables Mantenidas': variables_mantenidas})

# Mostrar resultados
print("Variables eliminadas:")
print(df_eliminadas)

print("\nVariables mantenidas:")
print(df_mantenidas)

# Revisión Lógica sobre las variables eliminadas de acuerdo al contexto del problema
variables_lógicas_vivienda = ["VPH_1DOR", "VPH_TINACO"]

# 1. Eliminar de variables_eliminadas y agregar a variables_mantenidas
for var in variables_lógicas_vivienda:
    if var in variables_eliminadas:
        variables_eliminadas.remove(var)
        if var not in variables_mantenidas:  # Evitar duplicados
            variables_mantenidas.append(var)

# 2. Actualizar DataFrames
df_eliminadas = pd.DataFrame({'Variables Eliminadas': variables_eliminadas})
df_mantenidas = pd.DataFrame({'Variables Mantenidas': variables_mantenidas})

# 3. Eliminar filas específicas por valor en df_mantenidas (opcional)
valores_a_eliminar = ["index"]  # Valores a eliminar
df_mantenidas = df_mantenidas[~df_mantenidas['Variables Mantenidas'].isin(valores_a_eliminar)]

print("\nVariables mantenidas:")
print(df_mantenidas)

data_clean_vivienda = data2.drop(columns=variables_eliminadas)
print(data_clean_vivienda.shape)
data_clean_vivienda.columns.tolist()

plt.figure(figsize=(100, 100))
sns.heatmap(data_clean_vivienda.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Mapa de Calor - Correlación de Variables con Indicador de Carencia Vivienda")
plt.show()

import numpy as np
import pandas as pd
from sklearn.feature_selection import mutual_info_regression

# Convertimos el DataFrame a un array de numpy
X = np.array(data_clean_vivienda)

# Número de características
n_features = X.shape[1]

# Matriz para almacenar la información mutua
mi_matrix = np.zeros((n_features, n_features))

# Iteramos sobre cada par de variables
for i in range(n_features):
    for j in range(i + 1, n_features):
        # Calculamos la información mutua entre la variable i y la j
        mi = mutual_info_regression(X[:, [i]], X[:, j])  # mutual_info_regression espera una matriz 2D
        mi_matrix[i, j] = mi
        mi_matrix[j, i] = mi  # La matriz es simétrica

# Convertimos la matriz a un DataFrame con los nombres de las columnas originales
mi_df = pd.DataFrame(mi_matrix, index=data_clean_vivienda.columns, columns=data_clean_vivienda.columns)

# Mostrar la matriz de información mutua
print("Matriz de información mutua:")
print(mi_df)

# Umbral de información mutua
umbral = 0.90

# Filtrar los pares con información mutua mayor al umbral
pares_alto_mi = [(var1, var2, mi_df.loc[var1, var2])
                 for var1 in mi_df.index for var2 in mi_df.columns
                 if var1 != var2 and mi_df.loc[var1, var2] > umbral]

# Convertir los resultados en un DataFrame
df_pares_alto_mi = pd.DataFrame(pares_alto_mi, columns=["Variable 1", "Variable 2", "Información Mutua"]).drop_duplicates()

# Imprimir solo la tabla filtrada
if not df_pares_alto_mi.empty:
    print("\nPares con Información Mutua mayor a 0.90:")
    print(df_pares_alto_mi)
else:
    print("\nNo se encontraron pares con MI > 0.90.")

# Eliminar la columna "P8A14AN" si existe en el DataFrame
data_clean_vivienda = data_clean_vivienda.drop(columns=["P8A14AN", "index"])

# Mostrar el DataFrame actualizado
print(data_clean_vivienda)

"""Modelos de Aprendizaje Supervisado Indicador Pobreza - Elias del Blanco"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
from sklearn.model_selection import train_test_split

# División de datos en entrenamiento y prueba
X = data_clean_pobreza.drop("pobreza_e_pob", axis=1)
y = data_clean_pobreza.pobreza_e_pob
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Definir función para probar hiperparámetros
def tune_model(model, param_grid, X_train, y_train, X_test, y_test):
    grid_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=0)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    print(f"Mejores parámetros para {model.__class__.__name__}: {grid_search.best_params_}")
    print(f"Nuevo R² Score: {r2_score(y_test, y_pred)}")
    print(f"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}")
    print(f"Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_test, y_pred))}\n")

    return best_model

# Hiperparámetros para RF
rf_params = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Hiperparámetros para GB
gb_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7]
}

# Hiperparámetros para MLP
mlp_params = {
    'hidden_layer_sizes': [(8,8), (16,16), (32,32)],
    'activation': ['relu', 'tanh'],
    'solver': ['adam', 'lbfgs'],
    'alpha': [0.0001, 0.001, 0.01]
}

# Hiperparámetros para DT
dt_params = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Hiperparámetros para KNN
knn_params = {
    'n_neighbors': [3, 5, 7, 10],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}

ridge_params = {'alpha': [0.1, 1, 10, 100]}

# Aplicar ajuste de hiperparámetros
best_rf = tune_model(RandomForestRegressor(random_state=0), rf_params, X_train, y_train, X_test, y_test)
best_gb = tune_model(GradientBoostingRegressor(random_state=0), gb_params, X_train, y_train, X_test, y_test)
best_mlp = tune_model(MLPRegressor(max_iter=500, random_state=0), mlp_params, X_train, y_train, X_test, y_test)
best_dt = tune_model(DecisionTreeRegressor(random_state=0), dt_params, X_train, y_train, X_test, y_test)
best_knn = tune_model(KNeighborsRegressor(), knn_params, X_train, y_train, X_test, y_test)
best_ridge = tune_model(Ridge(), ridge_params, X_train, y_train, X_test, y_test)

# LR no requiere ajuste de hiperparámetros
best_lr = LinearRegression()
best_lr.fit(X_train, y_train)
y_pred_lr = best_lr.predict(X_test)
print(f"R² Score para Linear Regression: {r2_score(y_test, y_pred_lr)}")
print(f"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_lr)}")
print(f"Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_test, y_pred_lr))}\n")

"""El modelo final para realizar la predicción sera: el modelo de Gradient Boosting.

Este modelo logró tener resultados de precisión excelentes:
R² Score: 0.939138677059258
Mean Absolute Error (MAE): 0.02600485782849423
Root Mean Squared Error (RMSE): 0.0339397701712702
"""